<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  
  <link rel="shortcut icon" href="../../../img/favicon.ico">
  <title>强化学习 - Huang Hao's Blog</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700" />

  <link rel="stylesheet" href="../../../css/theme.css" />
  <link rel="stylesheet" href="../../../css/theme_extra.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/github.min.css" />
  
  <script>
    // Current page data
    var mkdocs_page_name = "\u5f3a\u5316\u5b66\u4e60";
    var mkdocs_page_input_path = "\u5185\u5bb9\\\u5176\u4ed6\\\u5f3a\u5316\u5b66\u4e60.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../../../js/jquery-2.1.1.min.js" defer></script>
  <script src="../../../js/modernizr-2.8.3.min.js" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
        <a href="../../.." class="icon icon-home"> Huang Hao's Blog</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../../..">Huang Hao' Blog</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">内容</span></p>
                <ul class="current">
                    <li class="toctree-l1"><a class="reference internal" href="#">Paper阅读</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../paper%E9%98%85%E8%AF%BB/Bayesian%20network%20structure%20learning%20%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%E9%97%AE%E9%A2%98/">Bayesian network structure learning: 最短路径问题</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../paper%E9%98%85%E8%AF%BB/EM%20algorithm%20%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7%E7%AE%97%E6%B3%95/">EM algorithm 期望最大算法</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../paper%E9%98%85%E8%AF%BB/K2%E7%AE%97%E6%B3%95%E3%80%8AA%20Bayesian%20Method%20for%20the%20Induction%20of%20Probabilistic%20Networks%20from%20Data%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">K2算法《A Bayesian Method for the Induction of Probabilistic Networks from Data》阅读笔记</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../paper%E9%98%85%E8%AF%BB/Linear%20non-Gaussian%20Acyclic%20Model/">A Linear Non-Gaussian Acyclic Model for Causal Discovery</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../paper%E9%98%85%E8%AF%BB/%E3%80%8AA%20Survey%20of%20Learning%20Causality%20with%20Data%20Problems%20and%20Methods%E3%80%8B/">《A Survey of Learning Causality with Data: Problems and Methods》阅读笔记</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../paper%E9%98%85%E8%AF%BB/%E3%80%8AAtomistic%20Line%20Graph%20Neural%20Network%20for%20improved%20materials%20property%20predictions%E3%80%8B%20%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">《Atomistic Line Graph Neural Network for improved materials property predictions》 阅读笔记</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../paper%E9%98%85%E8%AF%BB/%E3%80%8ACAUSAL%20DISCOVERY%20WITH%20REINFORCEMENT%20LEARNING%E3%80%8B%20%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">《CAUSAL DISCOVERY WITH REINFORCEMENT LEARNING》 读书笔记</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../paper%E9%98%85%E8%AF%BB/%E3%80%8AExploiting%20Experts%E2%80%99%20Knowledge%20for%20Structure%20Learning%20of%20Bayesian%20Networks%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">《Exploiting Experts’ Knowledge for Structure Learning of Bayesian Networks》阅读笔记</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../paper%E9%98%85%E8%AF%BB/%E3%80%8AImproved%20K2%20algorithm%20for%20Bayesian%20network%20structure%20learning%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">《Improved K2 algorithm for Bayesian network structure learning》阅读笔记</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../paper%E9%98%85%E8%AF%BB/%E3%80%8ALearning%20to%20Optimize%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">《Learning to Optimize》阅读笔记</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../paper%E9%98%85%E8%AF%BB/%E3%80%8ANEURAL%20COMBINATORIAL%20OPTIMIZATION%20WITH%20REINFORCEMENT%20LEARNING%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">《NEURAL COMBINATORIAL OPTIMIZATION WITH REINFORCEMENT LEARNING》读书笔记</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../paper%E9%98%85%E8%AF%BB/%E3%80%8ANeural%20Machine%20Translation%20By%20Jointly%20Learning%20To%20Align%20And%20Translate%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">《Neural Machine Translation By Jointly Learning To Align And Translate》阅读笔记</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../paper%E9%98%85%E8%AF%BB/%E3%80%8APC%20algorithm%E3%80%8B%E5%AD%A6%E4%B9%A0/">《PC algorithm》学习</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../paper%E9%98%85%E8%AF%BB/%E3%80%8AThe%20max-min%20hill-climbing%20Bayesian%20network%20structure%20learning%20algorithm%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">《The max-min hill-climbing Bayesian network structure learning algorithm》阅读笔记</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../paper%E9%98%85%E8%AF%BB/%E3%80%8Aattention%20is%20all%20your%20need%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">《attention is all your need》阅读笔记</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../paper%E9%98%85%E8%AF%BB/%E3%80%8Adetermining%20the%20direction%20of%20the%20local%20search%20in%20topological%20ordering%20space%20for%20Bayesian%20network%20structure%20learning%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94/">《determining the direction of the local search in topological ordering space for Bayesian network structure learning》阅读笔记</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../paper%E9%98%85%E8%AF%BB/%E3%80%8AdirectLiGAM%E3%80%8B%E7%BB%93%E6%9E%84%E6%96%B9%E7%A8%8B%E6%A8%A1%E5%9E%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">《A Linear Non-Gaussian Acyclic Model for Causal Discovery》读书笔记</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../paper%E9%98%85%E8%AF%BB/%E3%80%8Apointer%20networks%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">《pointer networks》阅读笔记</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../paper%E9%98%85%E8%AF%BB/%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90%EF%BC%9A%E4%B8%89%E6%AC%A1%E6%A0%B7%E6%9D%A1%E6%8F%92%E5%80%BC/">数值分析：三次样条插值</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../paper%E9%98%85%E8%AF%BB/%E9%A2%91%E7%8E%87%E7%BB%9F%E8%AE%A1%E5%AD%A6VS%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BB%9F%E8%AE%A1%E5%AD%A6/">频率统计学VS贝叶斯统计学</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="#">《Neural Machine Translation By Jointly Learning To Align And Translate》阅读笔记.assets</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../paper%E9%98%85%E8%AF%BB/%E3%80%8ANeural%20Machine%20Translation%20By%20Jointly%20Learning%20To%20Align%20And%20Translate%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0.assets/BIC%20%E8%AF%84%E5%88%86/">BIC 评分</a>
                </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="#">《PC algorithm》学习.assets</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../paper%E9%98%85%E8%AF%BB/%E3%80%8APC%20algorithm%E3%80%8B%E5%AD%A6%E4%B9%A0.assets/%E3%80%8AStructure%20Learning%20of%20Bayesian%20Networks%20by%20Genetic%20Algorithms%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">《Structure Learning of Bayesian Networks by Genetic Algorithms》阅读笔记</a>
                </li>
    </ul>
                </li>
    </ul>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="#">Python</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../python/SPHINX%20%E4%BD%BF%E7%94%A8/">SPHINX 使用</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../python/argmax%EF%BC%8Cargsort%E7%9A%84%E4%BD%BF%E7%94%A8/">argmax，argsort的使用</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../python/conda%20%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/">conda 虚拟环境</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../python/datatime%20%E6%A8%A1%E5%9D%97/">datetime 模块</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../python/django/">django</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../python/flask%20%E5%B0%8F%E5%9E%8B%E5%8D%9A%E5%AE%A2/">flask 小型博客</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../python/google%20%E6%96%87%E6%A1%A3%E6%A0%87%E5%87%86/">Google 文档标准</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../python/keras.backend.rnn%E5%AD%A6%E4%B9%A0/">keras.backend.rnn学习</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../python/lru_cache%20%E8%A3%85%E9%A5%B0%E5%99%A8/">lru_cache 装饰器</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../python/np.cov%20%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5%E7%9A%84%E8%AE%A1%E7%AE%97/">np.cov 协方差矩阵的计算</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../python/python%20logging%20%E6%A0%87%E5%87%86%E5%BA%93/">python logging 标准库</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../python/python%20%E7%B1%BB%E4%B8%8E%E5%AF%B9%E8%B1%A1/">Python 类与对象</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../python/python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">Python学习笔记</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../python/pytorch%20%E5%AD%A6%E4%B9%A0/">pytorch 学习</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../python/super%28%29/">super()</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../python/tkinter%20%E5%AD%A6%E4%B9%A0/">tkinter 学习</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../python/try%E7%9A%84%E7%94%A8%E6%B3%95/">try的用法</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../python/%E3%80%8Acelery%E3%80%8B/">《celery》</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../python/%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0/">内置函数</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../python/%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0/">定义函数</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../python/%E6%A8%A1%E5%9D%97/">模块</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../python/%E6%A8%A1%E5%9D%97%E4%B8%8E%E5%8C%85/">模块与包</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../python/%E7%94%9F%E6%88%90%E5%99%A8/">生成器</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../python/%E7%B1%BB/">类</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../python/%E8%A3%85%E9%A5%B0%E5%99%A8/">装饰器</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../python/%E9%97%AD%E5%8C%85/">闭包</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="#">Session 与 cookie.assets</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../python/session%20%E4%B8%8E%20cookie.assets/session%20%E4%B8%8E%20cookie/">session 与 cookie</a>
                </li>
    </ul>
                </li>
    </ul>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="#">书籍阅读</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../%E4%B9%A6%E7%B1%8D%E9%98%85%E8%AF%BB/%E3%80%8A%E5%87%B8%E4%BC%98%E5%8C%96%E3%80%8B/">凸优化学习笔记</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../%E4%B9%A6%E7%B1%8D%E9%98%85%E8%AF%BB/%E3%80%8A%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E3%80%8B/">《操作系统》</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../%E4%B9%A6%E7%B1%8D%E9%98%85%E8%AF%BB/%E3%80%8A%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%8E%9F%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/">《深入浅出强化学习：原理入门》</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../%E4%B9%A6%E7%B1%8D%E9%98%85%E8%AF%BB/%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B/">深度学习</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../%E4%B9%A6%E7%B1%8D%E9%98%85%E8%AF%BB/%E3%80%8A%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">《算法导论》阅读笔记</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../%E4%B9%A6%E7%B1%8D%E9%98%85%E8%AF%BB/%E3%80%8A%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">《计算机网络》读书笔记</a>
                </li>
    </ul>
                    </li>
                    <li class="toctree-l1 current"><a class="reference internal current" href="#">其他</a>
    <ul class="current">
                <li class="toctree-l2"><a class="reference internal" href="../JavaScript/">JavaScript</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../Jekyll%20%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/">Jekyll 搭建博客</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../Nginx/">Nginx教程</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../R%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0/">R语言学习</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../bootstrap%E5%AD%A6%E4%B9%A0/">bootstrap学习</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../docker/">docker</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../git%20%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6/">git 版本控制</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../git/">git</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../htmlCSS/">html学习</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../http%E5%8D%8F%E8%AE%AE%E5%AD%A6%E4%B9%A0/">HTTP协议的学习</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../mysql%20%E5%AE%89%E8%A3%85/">mysql 安装</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../yum/">yum</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../%E3%80%8Adocker%E3%80%8B/">《docker》</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../%E4%BD%BF%E7%94%A8markdown%E5%88%B6%E4%BD%9Cppt/">slide 1</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../%E5%8A%9B%E6%89%A3%E9%A2%98%E7%9B%AE%E6%80%BB%E7%BB%93/">力扣题目总结</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../%E5%8F%82%E6%95%B0%E7%9A%84%E4%BC%A0%E9%80%92/">参数的传递</a>
                </li>
                <li class="toctree-l2 current"><a class="reference internal current" href="./">强化学习</a>
    <ul class="current">
    <li class="toctree-l3"><a class="reference internal" href="#_2">马尔科夫性</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#_3">马尔科夫过程</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#_4">马尔可夫决策过程</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#_5">模型</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#_6">强化学习适用的场景</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#_7">强化学习中的基本概念</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#_8">动作轨迹</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#return">return</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#value-function">value function</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#aiagent">AI如何控制agent</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#q-q-">q-函数，如何得到q-函数？</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#dqn">DQN</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#dqntd">如何训练DQN？使用TD算法</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#_9">策略函数</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#_10">动作价值函数</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#_11">状态价值函数</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#_12">学习策略函数</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#actor-critic">actor-critic 算法</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#_13">训练过程</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#_14">梯度计算与数学推导</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#_15">蒙特卡洛算法</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#_16">求积分</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#_17">数学推导</a>
    </li>
        </ul>
    </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../%E6%96%87%E4%BB%B6%E7%9A%84io/">文件的io</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/">线性代数</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../%E7%BA%BF%E7%A8%8B%E4%B8%8E%E8%BF%9B%E7%A8%8B/">线程与进程</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE%E2%80%94%E2%80%94%E5%88%9D%E8%AF%86/">网络协议——初识</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../%E7%BD%91%E9%A1%B5%E7%9A%84%E7%BB%84%E6%88%90/">网页的组成</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../%E8%8D%89%E7%A8%BF%E6%9C%AC/">Draft</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/">计算机网络</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/">设计模式</a>
                </li>
    </ul>
                    </li>
                </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../../..">Huang Hao's Blog</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../..">Docs</a> &raquo;</li>
    
      
        
          <li>内容 &raquo;</li>
        
      
        
          <li>其他 &raquo;</li>
        
      
    
    <li>强化学习</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  
  <hr/>
</div>

          <div role="main">
            <div class="section">
              
                <h1 id="_1">强化学习</h1>
<ul>
<li>无监督学习</li>
<li>有监督学习</li>
<li>强化学习</li>
</ul>
<p><img alt="image-20211020104852274" src="../%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20211020104852274.png" /></p>
<h2 id="_2">马尔科夫性</h2>
<p>马尔可夫性指的是系统的下一个状态$s_{t+1}$仅与当前状态$s_{t}$有关，而与以前的状态无关。
$$
p(s_{t+1}|s_t)=p(s_{t+1}|s_1,...,s_t)
$$</p>
<h2 id="_3">马尔科夫过程</h2>
<p>马尔科夫过程是一个二元组$(S,P)$, 且满足：s是有限状态集合，p是状态转移概率。状态转移矩阵为：
$$
P=\left[\begin{array}{ccc}
P_{11} &amp; \cdots &amp; P_{1 n} \
\vdots &amp; \vdots &amp; \vdots \
P_{n 1} &amp; \cdots &amp; P_{n n}
\end{array}\right]
$$</p>
<h2 id="_4">马尔可夫决策过程</h2>
<p>$(S,A,P,R,\lambda)$ </p>
<p>(状态集，动作集，状态转移矩阵，回报函数，折扣因子)</p>
<p>强化学习的目的是：给定一个马尔科夫决策过程，勋章最优策略，通常用$\pi$表示。</p>
<h2 id="_5">模型</h2>
<p>马尔可夫决策过程 -----核心</p>
<p>​       -动态规划 Neuro-Dynamic Programming</p>
<p>​       -近似动态规划 Approximate dynamic programming</p>
<p>​       -蒙特卡洛</p>
<p>隐马尔科夫模型</p>
<h2 id="_6">强化学习适用的场景</h2>
<ol>
<li>试错探索，需要探索环境来获取对环境的理解</li>
<li>agent会从环境中获得延迟的奖励</li>
<li>数据是时间序列的，而不是独立同分布的</li>
<li>agent的行为会影响它随后得到的数据</li>
</ol>
<h2 id="_7">强化学习中的基本概念</h2>
<p>t时刻的状态：
$$
s_t
$$
t时刻的动作：
$$
a_t
$$
t时刻采取了动作以后，获得的奖励：
$$
r_t
$$
<img alt="image-20211020155059699" src="../%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20211020155059699.png" /></p>
<p><strong>回报</strong>：回报是对于当前时刻t来说，未来获得的所有奖励的总和，只有给定了policy才是可以计算出来的。
$$
U_t = r_t+r_{t+1}+r_{t+2}+...
$$
policy：动作策略函数
$$
\pi(s)
$$
<strong>价值函数</strong>：q-function，value-function
$$
Q_{\pi}\left(s_{t}, a_{t}\right)=\mathbb{E}\left[U_{t} \mid S_{t}=s_{t}, A_{t}=a_{t}\right]
$$</p>
<blockquote>
<p>价值函数本质上是与policy相关联的，因为回报的计算是需要指定policy的。由于状态变化的随机性，和policy函数的随机性，一般写为期望的形式。</p>
</blockquote>
<p><strong>状态价值函数</strong>：state-value function
$$
V(s ; \boldsymbol{\theta})=\sum_{a} \pi\left(\left.a\right|<em>{s} ; \boldsymbol{\theta}\right) \cdot Q</em>{\pi}(s, a)
$$</p>
<blockquote>
<p>状态价值函数也是同理，其实本质上是与policy函数有所关联的。</p>
</blockquote>
<p>智能体类型：</p>
<p>value-based： 显式地拟合q-function，policy可以通过q-function得到</p>
<p>policy-based：显式地拟合状态价值函数</p>
<p>actor-critic: 显式地拟合policy与value，也是目前最常用的。</p>
<h2 id="_8">动作轨迹</h2>
<p>s, a, r, s, a, r ...</p>
<p><img alt="image-20211020155059699" src="../%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20211020155059699.png" /></p>
<h2 id="return">return</h2>
<p>考虑对未来的估计</p>
<p><img alt="image-20211020153735985" src="../%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20211020153735985.png" /></p>
<p><img alt="image-20211020153808614" src="../%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20211020153808614.png" /></p>
<p>注意R也是具有随机性的。</p>
<h2 id="value-function">value function</h2>
<p>因为$U_t$具有随机性，并不能在t时刻准确计算出了，我们对其求期望，则得到：</p>
<p><img alt="image-20211020154239955" src="../%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20211020154239955.png" /></p>
<p>Q-函数非常重要。</p>
<h2 id="aiagent">AI如何控制agent</h2>
<p>学习 policy $\pi$</p>
<p>学习 q-函数</p>
<p><img alt="image-20211020154741986" src="../%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20211020154741986.png" /></p>
<h2 id="q-q-">q-函数，如何得到q-函数？</h2>
<p><img alt="image-20211021122432812" src="../%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20211021122432812.png" /></p>
<p>可以用神经网络近似Q-函数</p>
<h3 id="dqn">DQN</h3>
<p><img alt="image-20211021122524885" src="../%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20211021122524885.png" /></p>
<h3 id="dqntd">如何训练DQN？使用TD算法</h3>
<p><img alt="image-20211021123201775" src="../%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20211021123201775.png" /> </p>
<p><img alt="image-20211021133129468" src="../%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20211021133129468.png" /></p>
<h2 id="_9">策略函数</h2>
<p>给定状态，动作的分布概率。
$$
\pi(a \mid s)
$$
如果s有限，那么我们可以用表格的方式进行训练。但是s的状态空间非常庞大，那么就需要采用近似的方法。</p>
<h3 id="_10">动作价值函数</h3>
<h3 id="_11">状态价值函数</h3>
<h3 id="_12">学习策略函数</h3>
<p><img alt="image-20211021133812818" src="../%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20211021133812818.png" /></p>
<p><strong>策略梯度 policy gradient</strong></p>
<p><img alt="image-20211021134058593" src="../%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20211021134058593.png" /></p>
<h2 id="actor-critic">actor-critic 算法</h2>
<p>用两个神经网络分别近似 价值函数，策略函数。</p>
<p>actor：</p>
<p><img alt="image-20211021134532606" src="../%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20211021134532606.png" /></p>
<p>critic</p>
<p><img alt="image-20211021134548689" src="../%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20211021134548689.png" /></p>
<p><img alt="image-20211021134623985" src="../%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20211021134623985.png" /></p>
<p><img alt="image-20211021134654262" src="../%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20211021134654262.png" /></p>
<p>critic 输出实数。</p>
<p><img alt="image-20211021135042075" src="../%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20211021135042075.png" /></p>
<p>V(s) 状态价值函数：用于评价当前状态下的策略函数，其中策略函数可以写为：
$$
\pi(a \mid s ; \boldsymbol{\theta})
$$
它的参数为$\theta$ 。然后V(s)是根据q函数来评价策略函数的，所以我们也希望获得一个准确的q函数：
$$
q(s, a ; \mathbf{w})
$$
q函数也是要通过训练的，q函数需要从环境中得到训练。</p>
<p>总的过程可以理解为：环境与agent交互所产生的数据训练出一个越来越准确的q函数，通过q函数不断地优化$\pi()$策略函数。 </p>
<h3 id="_13">训练过程</h3>
<ol>
<li>观察状态 $S_t$</li>
<li>根据当前策略函数得到$a_t$，与$r_t$</li>
<li>一直执行下去，直到得到所有的$r_t,r_{t+1},...,r_{n}=U_t$</li>
<li>已经有了 $U_t$ ，那么更新 参数 w</li>
<li>再用新的q函数去优化 $\pi()$策略函数。</li>
</ol>
<h3 id="_14">梯度计算与数学推导</h3>
<p>暂时不考虑，先了解框架</p>
<h2 id="_15">蒙特卡洛算法</h2>
<p>通过随机样本估计真实值。</p>
<p>大数定律是理论基础。</p>
<p>两大功能：1. 求期望。2. 求积分</p>
<h3 id="_16">求积分</h3>
<p><img alt="image-20211022140013180" src="../%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20211022140013180.png" /></p>
<p><img alt="image-20211022140109401" src="../%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20211022140109401.png" /></p>
<p>求定积分的时候就直接可以用蒙特卡洛算法求解。</p>
<p>该案例只说明了一元函数，实际上多元函数同样适用。</p>
<p><img alt="image-20211022140248427" src="../%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20211022140248427.png" /></p>
<p>一般要求$\Omega$的体积好求。</p>
<p><strong>计算期望</strong></p>
<p>概率密度函数
$$
p(x)
$$
如果$f(x)$是x的函数，那么$f(x)$的期望可以写为：
$$
\mathbb{E}<em>{X \sim p}[f(X)]=\int</em>{\mathbb{R}^{d}} f(\mathbf{x}) \cdot p(\mathbf{x}) d \mathbf{x}
$$
这就需要使用蒙特卡罗方法。</p>
<p><img alt="image-20211022140804036" src="../%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20211022140804036.png" /></p>
<h3 id="_17">数学推导</h3>
<p>蒙特卡洛方法是一种可以近似计算任意函数积分的数值方法，并不是一个求期望的方法（只是它常用于求期望）。</p>
<p>根据大数定律，样本均值<a href="https://zh.wikipedia.org/wiki/依概率收敛">依概率收敛</a>于期望值。得到
$$
E[f(x)]=\int_{x \in S} f(x) p(x) d x \approx \frac{1}{N} \sum_{i=1}^{N} f\left(x^{i}\right)
$$
记$g(x)=f(x)p(x)$ , 带入可得：
$$
\int_{x \in S} g(x) d x \approx \frac{1}{N} \sum_{i=1}^{N} \frac{g\left(x^{i}\right)}{p\left(x^{i}\right)}
$$
$E[g(x)]$为：
$$
\begin{aligned}
E\left[\frac{1}{N} \sum_{i=1}^{N} \frac{g\left(x^{i}\right)}{p\left(x^{i}\right)}\right] &amp;=\frac{1}{N} E\left[\sum_{i=1}^{N} \frac{g\left(x^{i}\right)}{p\left(x^{i}\right)}\right] \
&amp;=\frac{1}{N} N \times \int \frac{g(x)}{p(x)} p(x) d x \
&amp;=\int g(x) d x
\end{aligned}
$$
得到：
$$
E[g(x)] =\frac {1}{N} \sum_{i=1}^{N} g\left(x^{i}\right)=\int g(x) d x
$$</p>
<h1 id="_18">概念</h1>
<h2 id="sequential-decision-making">Sequential Decision Making</h2>
<p>序列决策过程</p>
<p>agent与environment进行交互，得到一个历史：
$$
\mathrm{H}<em>{\mathrm{t}}=\mathrm{O}</em>{1}, \mathrm{R}<em>{1}, \mathrm{~A}</em>{1}, \ldots, \mathrm{A}<em>{\mathrm{t}-1}, \mathrm{O}</em>{\mathrm{t}}, \mathrm{R}<em>{\mathrm{t}}
$$
状态是历史的函数：
$$
\mathrm{S}</em>{\mathrm{t}}=\mathrm{f}\left(\mathrm{H}_{\mathrm{t}}\right)
$$
通常，观测是一个实值向量，矩阵。</p>
<h3 id="_19">马尔可夫决策过程</h3>
<p>环境是完全可观测的。</p>
<h2 id="action-spaces">action spaces</h2>
<ul>
<li>离散动作空间</li>
<li>连续动作空间</li>
</ul>
<h2 id="_20">强化学习的主要组成成分</h2>
<ol>
<li>策略函数，agent会用这个函数来选取下一步的动作。</li>
<li>价值函数，用价值函数来评估当前状态。agent 现在进入这个状态，对你后面的收益能带来多大的影响。</li>
<li>模型，用于表示agent对环境的理解。</li>
</ol>
<h2 id="policy">policy</h2>
<p>policy是agent的行为模型，它决定了这个agent的行为，分为两种：</p>
<ol>
<li>随机策略</li>
<li>决定性策略</li>
</ol>
<h2 id="value-function_1">value function</h2>
<p>价值函数是对未来奖励的估计，用于判断agent 现在进入该状态能获得多大的收益。</p>
<p>eg. q-函数</p>
<h2 id="model">model</h2>
<p>模型决定了下一个状态是怎么样的。</p>
<h2 id="agent">agent 分类</h2>
<ul>
<li>基于价值的</li>
<li>基于策略的</li>
<li>混合 actor-critic agent</li>
</ul>
<h2 id="model_1">model</h2>
<ul>
<li>有模型</li>
<li>免模型</li>
</ul>
<p>对于马尔可夫决策过程，四元组：状态，动作，转移函数，奖励函数。如果agent知道转移函数和奖励函数，那么就是有型学习。
$$
&lt;\mathrm{S}, \mathrm{A}, \mathrm{P}, \mathrm{R}&gt;
$$</p>
<h2 id="_21">马尔可夫决策过程 详解</h2>
<h3 id="_22">马尔可夫过程</h3>
<ul>
<li>马尔可夫性质：如果一个状态转移是符合马尔可夫的，那就是说一个状态的下一个状态只取决于它当前状态，而跟它当前状态之前的状态都没有关系。</li>
</ul>
<p>$$
\mathrm{p}\left(\mathrm{s}<em>{\mathrm{t}+1} \mid \mathrm{s}</em>{\mathrm{t}}\right)=\mathrm{p}\left(\mathrm{s}<em>{\mathrm{t}+1} \mid \mathrm{h}</em>{\mathrm{t}}\right)
$$</p>
<ul>
<li>马尔可夫过程</li>
</ul>
<p><img alt="image-20211020122043033" src="../%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20211020122043033.png" /></p>
<p>用状态转移矩阵来描述各个状态之间的变化：</p>
<p><img alt="image-20211020122121567" src="../%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20211020122121567.png" /></p>
<h2 id="_23">马尔可夫奖励过程</h2>
<p>当达到一个状态时，可以获得相应的奖励</p>
<h2 id="bellman-equation">Bellman Equation</h2>
<p><img alt="image-20211020132030031" src="../%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20211020132030031.png" /></p>
<ul>
<li>s′ 可以看成未来的所有状态。</li>
<li>转移 P(s'|s)P(s′∣s) 是指从当前状态转移到未来状态的概率。</li>
<li>V(s')V(s′) 代表的是未来某一个状态的价值。我们从当前这个位置开始，有一定的概率去到未来的所有状态，所以我们要把这个概率也写上去，这个转移矩阵也写上去，然后我们就得到了未来状态，然后再乘以一个 γ，这样就可以把未来的奖励打折扣。</li>
<li>第二部分可以看成是未来奖励的折扣总和(Discounted sum of future reward)。</li>
</ul>
<p><img alt="image-20211020132946975" src="../%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20211020132946975.png" /></p>
<p>Bellman Equation 定义了当前状态与未来状态的迭代关系。</p>
<h2 id="_24">马尔可夫决策过程</h2>
<p>状态转移矩阵：
$$
\mathrm{P}\left(\mathrm{s}<em>{\mathrm{t}+1}=\mathrm{s}^{\prime} \mid \mathrm{s}</em>{\mathrm{t}}=\mathrm{s}, \mathrm{a}<em>{\mathrm{t}}=\mathrm{a}\right)
$$
价值函数也改写为：
$$
\mathrm{R}\left(\mathrm{s}</em>{\mathrm{t}}=\mathrm{s}, \mathrm{a}<em>{\mathrm{t}}=\mathrm{a}\right)=\mathbb{E}\left[\mathrm{r}</em>{\mathrm{t}} \mid \mathrm{s}<em>{\mathrm{t}}=\mathrm{s}, \mathrm{a}</em>{\mathrm{t}}=\mathrm{a}\right]
$$
policy
$$
\pi(\mathrm{a} \mid \mathrm{s})=\mathrm{P}\left(\mathrm{a}<em>{\mathrm{t}}=\mathrm{a} \mid \mathrm{s}</em>{\mathrm{t}}=\mathrm{s}\right)
$$</p>
<h2 id="mrpmdp">MRP与MDP比较：</h2>
<p><img alt="image-20211020134541265" src="../%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20211020134541265.png" /></p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../%E6%96%87%E4%BB%B6%E7%9A%84io/" class="btn btn-neutral float-right" title="文件的io">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../%E5%8F%82%E6%95%B0%E7%9A%84%E4%BC%A0%E9%80%92/" class="btn btn-neutral" title="参数的传递"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../%E5%8F%82%E6%95%B0%E7%9A%84%E4%BC%A0%E9%80%92/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../%E6%96%87%E4%BB%B6%E7%9A%84io/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script>var base_url = '../../..';</script>
    <script src="../../../js/theme_extra.js" defer></script>
    <script src="../../../js/theme.js" defer></script>
      <script src="../../../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
