<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  
  <link rel="shortcut icon" href="../../img/favicon.ico">
  <title>《attention is all your need》阅读笔记 - Huang Hao's Blog</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700" />

  <link rel="stylesheet" href="../../css/theme.css" />
  <link rel="stylesheet" href="../../css/theme_extra.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/github.min.css" />
  
  <script>
    // Current page data
    var mkdocs_page_name = "\u300aattention is all your need\u300b\u9605\u8bfb\u7b14\u8bb0";
    var mkdocs_page_input_path = "Paper Readings\\\u300aattention is all your need\u300b\u9605\u8bfb\u7b14\u8bb0.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js" defer></script>
  <script src="../../js/modernizr-2.8.3.min.js" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> Huang Hao's Blog</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../..">Huang Hao' Blog</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Paper Readings</span></p>
                <ul class="current">
                    <li class="toctree-l1"><a class="reference internal" href="../%E3%80%8AA%20Survey%20of%20Learning%20Causality%20with%20Data%20Problems%20and%20Methods%E3%80%8B/">《A Survey of Learning Causality with Data: Problems and Methods》阅读笔记</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../%E3%80%8AStructure%20Learning%20of%20Bayesian%20Networks%20by%20Genetic%20Algorithms%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">《Structure Learning of Bayesian Networks by Genetic Algorithms》阅读笔记</a>
                    </li>
                    <li class="toctree-l1 current"><a class="reference internal current" href="./">《attention is all your need》阅读笔记</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#abstract">abstract</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#intro">intro</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#background">background</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#model-architecture">model architecture</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#encoder-and-decoder-stacks">encoder and decoder stacks</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#attention">attention</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scaled-attention">scaled 点乘 attention</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#_1">多头注意力</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#_2">总结</a>
    </li>
    </ul>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">算法基础</span></p>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../../%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80/python%E7%9A%84%E8%AF%AD%E8%A8%80%E7%89%B9%E6%80%A7/">python的语言特性</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../../%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80/%E3%80%8A%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">《算法导论》阅读笔记</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../../%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80/%E5%8A%9B%E6%89%A3%E9%A2%98%E7%9B%AE%E6%80%BB%E7%BB%93/">力扣题目总结</a>
                    </li>
                </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">Huang Hao's Blog</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
        
          <li>Paper Readings &raquo;</li>
        
      
    
    <li>《attention is all your need》阅读笔记</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  
  <hr/>
</div>

          <div role="main">
            <div class="section">
              
                <h1 id="attention-is-all-your-need">《attention is all your need》阅读笔记</h1>
<p><img alt="image-20211120192319481" src="../%E3%80%8Aattention%20is%20all%20your%20need%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0.assets/image-20211120192319481.png" /></p>
<p>这是一篇深度学习领域的重要文章，来自谷歌。</p>
<h2 id="abstract">abstract</h2>
<p>主要的句子翻译模型主要是基于复杂的循环神经网络或者卷积神经网络，包含了编码器解码器结构。目前最好性能的模型使用attention机制来连接编码器解码器。本文提出了Transformer，简单的网络结构，只依据attention机制，彻底摒弃了循环和卷积的网络结构。</p>
<p>表现：1. 更加利于并行。2. 比当前最好的翻译模型好2BLEU。</p>
<h2 id="intro">intro</h2>
<p>RNN。</p>
<p>LSTM。</p>
<p>gated recurrent neural networks。</p>
<p>attention。</p>
<h2 id="background">background</h2>
<p>略</p>
<h2 id="model-architecture">model architecture</h2>
<p>输入是一个序列：
$$
x_1,x_2,...,x_n
$$
encoder输出为：
$$
z_1,z_2,...,z_n
$$
给定了$[z_1,..,z_n]$, 解码器得到：
$$
y_1,y_2,...,y_m
$$
transformer 也服从这样的整体构造，下面查看细节：</p>
<h3 id="encoder-and-decoder-stacks">encoder and decoder stacks</h3>
<p><strong>encoder：</strong></p>
<p><img alt="image-20211120193633005" src="../%E3%80%8Aattention%20is%20all%20your%20need%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0.assets/image-20211120193633005.png" /></p>
<p>编码器由N（N=6）个这样的结构组成，第一个是多头注意力，第二个是前馈网络（多层感知机）。</p>
<p><strong>decoder：</strong></p>
<p><img alt="image-20211120193930882" src="../%E3%80%8Aattention%20is%20all%20your%20need%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0.assets/image-20211120193930882.png" /></p>
<p>解码器是N（N=6)个这样的结构组成，encoder的输出先进入到masked 多头注意力，然后在进入到和编码器相同的两个结构中去。</p>
<h3 id="attention">attention</h3>
<p>attention函数：将一个query和一系列的key-value pairs映射到输出。</p>
<h4 id="scaled-attention">scaled 点乘 attention</h4>
<p><img alt="image-20211120200301528" src="../%E3%80%8Aattention%20is%20all%20your%20need%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0.assets/image-20211120200301528.png" /></p>
<p>Q：是一个固定维度（$d_k$）的向量，在这里一般向量是 $1 \times d_k$的矩阵</p>
<p>K：也是固定维（$d_k$）度的向量</p>
<p>V：固定维度（$d_v$）的向量</p>
<p>如果把向量全部打包到一起，那么计算公式写为：</p>
<p><img alt="image-20211120200734281" src="../%E3%80%8Aattention%20is%20all%20your%20need%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0.assets/image-20211120200734281.png" /></p>
<p>在此，只需要了解这样的attention模块需要QKV三个输入，最终得到一个输出。</p>
<p>对于一个样本来说, attention 函数的写法为：
$$
QK^T=一个标量\
(QK^T)V= 1 \times d_v 的向量
$$
对于n个样本来说：Q就变为 $n \times d_k$ 的；K, V分别为：$m \times d_k$，$m \times d_v$</p>
<h4 id="_1">多头注意力</h4>
<p><img alt="image-20211120205925212" src="../%E3%80%8Aattention%20is%20all%20your%20need%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0.assets/image-20211120205925212.png" /></p>
<p>V 是一个向量进来，用$W^v$做一个线性变换，得到 $d_v$维度的向量</p>
<p>K 进来，用$W^k$做一个线性变换，得到$d_k$维度的向量</p>
<p>Q 进来，用$W^Q$做一个线性变换，同样一个得到$d_k$维度的向量</p>
<p>这样的话就是得到了一个head，计算出8个头（多头注意力，可以并行）</p>
<p>因为 <strong>scaled 点乘 attention</strong> 出来是一个 $d_k$维度的向量，所以就得到了 8 个 $d_k$ 维度的向量。每一个$d_k$ 是64 维度，所以这样就能得到 512 维的向量。</p>
<h2 id="_2">总结</h2>
<p><img alt="image-20211120213257937" src="../%E3%80%8Aattention%20is%20all%20your%20need%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0.assets/image-20211120213257937.png" /></p>
<ol>
<li>
<p>一个句子进来，通过分词和embedding得到, 假设是n个词，每个embedding向量为512维度：[n, 512] 。</p>
</li>
<li>
<p>位置编码，根据文中的公式，每个位置（1,2,...,n) 都有对应的位置编码，位置编码为 [1，512] 的向量。对应位置相加，得到 [n,512]。</p>
</li>
<li>
<p>进入到多头注意力模块。此时Q,K,V 都相同，都是 [n,512]。</p>
</li>
</ol>
<p><img alt="image-20211120205925212" src="../%E3%80%8Aattention%20is%20all%20your%20need%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0.assets/image-20211120205925212.png" /></p>
<ol>
<li>
<p>对V [n,512]，做一个线性映射，得到 [n,64]; 同样 对K，和Q做一样的操作，但是线性映射的权重不同。都是 [n,64]。这样的操作就得到了一个head: Q,K,V 。</p>
</li>
<li>
<p>因为一个head里面的QKV都是[n,64]，执行8次第四步（这8次的参数都是独立的），得到了8个QKV，其中每个QKV都是 [n,64]</p>
</li>
</ol>
<p><img alt="image-20211120200301528" src="../%E3%80%8Aattention%20is%20all%20your%20need%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0.assets/image-20211120200301528.png" /></p>
<p><img alt="image-20211120200734281" src="../%E3%80%8Aattention%20is%20all%20your%20need%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0.assets/image-20211120200734281.png" /></p>
<ol>
<li>对于一个head来说，进入到<strong>scaled点乘注意力结构</strong>中，得到了 [n,64] 的输出。</li>
<li>现在有8个头，得到了8 个[n,64]的输出，将它们concat到一起，得到[n,512] </li>
<li>又经过一个线性变换，他还是输出 [n,512]。</li>
<li>进入到 add&amp;norm层，维度不变</li>
<li>进入到多层感知机，隐藏层个数为2048，用ReLU激活，输出层512.所以结果还是[n,512]</li>
<li>第一步到第十步重复执行6次，得到最终编码器输出的[n,512]</li>
<li>复制三份[n,512]，两个进入到解码器的对应位置。</li>
</ol>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../../%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80/python%E7%9A%84%E8%AF%AD%E8%A8%80%E7%89%B9%E6%80%A7/" class="btn btn-neutral float-right" title="python的语言特性">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../%E3%80%8AStructure%20Learning%20of%20Bayesian%20Networks%20by%20Genetic%20Algorithms%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" class="btn btn-neutral" title="《Structure Learning of Bayesian Networks by Genetic Algorithms》阅读笔记"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../%E3%80%8AStructure%20Learning%20of%20Bayesian%20Networks%20by%20Genetic%20Algorithms%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../../%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80/python%E7%9A%84%E8%AF%AD%E8%A8%80%E7%89%B9%E6%80%A7/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script>var base_url = '../..';</script>
    <script src="../../js/theme_extra.js" defer></script>
    <script src="../../js/theme.js" defer></script>
      <script src="../../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
