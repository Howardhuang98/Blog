
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.3.0, mkdocs-material-8.2.8">
    
    
      
        <title>深度学习 - Huang Hao's Blog</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.644de097.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.e6a45f82.min.css">
        
      
    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="none" data-md-color-accent="none">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#_1" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Huang Hao&#39;s Blog" class="md-header__button md-logo" aria-label="Huang Hao's Blog" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Huang Hao's Blog
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              深度学习
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Huang Hao&#39;s Blog" class="md-nav__button md-logo" aria-label="Huang Hao's Blog" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    Huang Hao's Blog
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        Huang Hao' Blog
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_2">
          书籍阅读
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="书籍阅读" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          书籍阅读
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../%E3%80%8A%E5%87%B8%E4%BC%98%E5%8C%96%E3%80%8B/" class="md-nav__link">
        凸优化学习笔记
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../%E3%80%8A%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E3%80%8B/" class="md-nav__link">
        《操作系统》
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../%E3%80%8A%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%8E%9F%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/" class="md-nav__link">
        《深入浅出强化学习：原理入门》
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          深度学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        深度学习
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    数学基础
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../%E3%80%8A%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" class="md-nav__link">
        《算法导论》阅读笔记
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../%E3%80%8A%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8%E3%80%8B/" class="md-nav__link">
        《线性代数及其应用》
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B/" class="md-nav__link">
        《统计学习方法》
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../%E3%80%8A%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/" class="md-nav__link">
        《计算机网络》读书笔记
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../%E3%80%8A%E8%BF%90%E7%AD%B9%E5%AD%A6%E6%95%99%E7%A8%8B%E3%80%8B/" class="md-nav__link">
        《运筹学教程》
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    数学基础
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                


<h1 id="_1">深度学习</h1>
<div class="toc">
<ul>
<li><a href="#_1">深度学习</a><ul>
<li><a href="#_2">数学基础</a></li>
</ul>
</li>
<li><a href="#tensorflow">tensorflow</a><ul>
<li><a href="#_3">一些基本概念</a></li>
<li><a href="#_4">张量</a><ul>
<li><a href="#_5">如何理解高维张量</a></li>
<li><a href="#slicing">索引与slicing</a></li>
<li><a href="#reshape">reshape</a></li>
<li><a href="#broadcasting">broadcasting 特征</a></li>
<li><a href="#_6">矩阵乘法</a></li>
<li><a href="#_7">小知识点：</a></li>
</ul>
</li>
<li><a href="#_8">常用函数</a><ul>
<li><a href="#tfvariable">tf.Variable</a></li>
<li><a href="#_9">常用运算函数</a></li>
<li><a href="#_10">输入标签，标签</a></li>
<li><a href="#tfgradienttape">tf.GradientTape</a></li>
<li><a href="#tfone_hot">tf.one_hot</a></li>
<li><a href="#tfargmax">tf.argmax</a></li>
</ul>
</li>
<li><a href="#modelfit">model.fit()</a><ul>
<li><a href="#generatorx">用generator来做输入：x</a></li>
</ul>
</li>
<li><a href="#tfdata">tf.data</a><ul>
<li><a href="#_11">从内存中加载数据</a></li>
<li><a href="#tfdatadatasetelement_spec">tf.data.Dataset.element_spec</a></li>
<li><a href="#datasetbatch">Dataset.batch</a></li>
</ul>
</li>
<li><a href="#tfnest">tf.nest</a><ul>
<li><a href="#functions">functions</a></li>
</ul>
</li>
<li><a href="#tfnestflatten">tf.nest.flatten</a></li>
<li><a href="#graph">graph</a></li>
<li><a href="#keras">keras</a><ul>
<li><a href="#sequential-model">sequential model</a></li>
<li><a href="#_12">网络结构</a></li>
<li><a href="#modelcompile">model.compile</a></li>
<li><a href="#modelfit_1">model.fit</a></li>
<li><a href="#buildself-inputs_shape">build(self, inputs_shape)</a></li>
<li><a href="#kerasbackendrnn">keras.backend.rnn</a></li>
</ul>
</li>
<li><a href="#_13">模型的保存与加载</a><ul>
<li><a href="#_14">全部保存</a></li>
<li><a href="#_15">保存结构</a></li>
<li><a href="#_16">保存权重，参数</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#pytorch">pytorch</a><ul>
<li><a href="#_17">准备工作</a></li>
<li><a href="#torchutilsdatadataset">torch.utils.data.Dataset</a><ul>
<li><a href="#_18">自定义数据集</a></li>
</ul>
</li>
<li><a href="#torchutilsdatadataloader">torch.utils.data.DataLoader</a></li>
<li><a href="#nnmodule">nn.module</a></li>
<li><a href="#nnmodulelayer">nn.module中的layer</a></li>
<li><a href="#_19">常用函数</a></li>
<li><a href="#_20">训练网络</a></li>
<li><a href="#_21">反向传播</a><ul>
<li><a href="#_22">计算图：</a></li>
<li><a href="#_23">微积分中的链式法则</a></li>
<li><a href="#_24">第一个版本的反向传播</a></li>
<li><a href="#_25">用例子来说明反向传播算法</a></li>
<li><a href="#_26">真正地考虑一个神经网络模型：</a></li>
<li><a href="#reference">reference</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#rnn">RNN循环神经网络</a><ul>
<li><a href="#rnn_1">经典的RNN结构</a></li>
<li><a href="#_27">优缺点</a></li>
<li><a href="#_28">缺点：</a></li>
<li><a href="#_29">损失函数：</a></li>
<li><a href="#reference_1">reference</a></li>
<li><a href="#sequence-to-sequence">sequence to sequence 结构</a><ul>
<li><a href="#_30">分析</a></li>
<li><a href="#teacher-forcing">teacher forcing</a></li>
<li><a href="#attention">attention</a><ul>
<li><a href="#keraslayerssimplernn">keras.layers.SimpleRNN</a></li>
</ul>
</li>
<li><a href="#simplernn">SimpleRNN 的输入输出</a></li>
<li><a href="#units">units到底是什么呢</a></li>
</ul>
</li>
<li><a href="#lstm">LSTM</a><ul>
<li><a href="#cell-state">cell state</a></li>
<li><a href="#keraslayerslstm">keras.layers.LSTM</a></li>
<li><a href="#tensorflow2-lstm">tensorflow2 的 LSTM 源码阅读</a><ul>
<li><a href="#lstmcell">LSTMCell 类</a></li>
<li><a href="#lstm_1">LSTM 类</a></li>
<li><a href="#transformer">transformer中的位置编码</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<h2 id="_2">数学基础</h2>
<p>标量</p>
<p>向量</p>
<p>矩阵</p>
<p>张量</p>
<h1 id="tensorflow">tensorflow</h1>
<h2 id="_3">一些基本概念</h2>
<ul>
<li>
<p>keras是tensorflow的高阶api，相当于集成好了一些tensor的操作。</p>
</li>
<li>
<p>tensorflow是张量的图计算流</p>
</li>
</ul>
<h2 id="_4">张量</h2>
<p>张量是一个高维的数组。</p>
<p>张量可以是大于2个axes的：</p>
<p><img alt="image-20211023210722469" src="../tensorflow.assets/image-20211023210722469.png" /></p>
<p><img alt="image-20211023210742352" src="../tensorflow.assets/image-20211023210742352.png" /></p>
<h3 id="_5">如何理解高维张量</h3>
<p>(3,2,5) 就是3个2*5的矩阵叠在一起</p>
<p>(5,256,256,3) 就是5张RGB照片叠在一起</p>
<p><img alt="image-20211023211129958" src="../tensorflow.assets/image-20211023211129958.png" /></p>
<p>因为常常处理图像数据，所以常用的tensor形状为(B,W,H,C)</p>
<h3 id="slicing">索引与slicing</h3>
<p>索引就如numpy的索引方式，有几个axes就能在几个axes上进行索引。</p>
<p>tensor[axe1,axe2,axe3]</p>
<p>对于高维向量的索引，如果直对一个维度进行索引，切片的样子很难想象出来，这里举例：</p>
<pre><code class="language-python"># There can be an arbitrary number of
# axes (sometimes called &quot;dimensions&quot;)
rank_3_tensor = tf.constant([
  [[0, 1, 2, 3, 4],
   [5, 6, 7, 8, 9]],
  [[10, 11, 12, 13, 14],
   [15, 16, 17, 18, 19]],
  [[20, 21, 22, 23, 24],
   [25, 26, 27, 28, 29]],])

print(rank_3_tensor)
&gt;&gt;&gt;
tf.Tensor(
[[[ 0  1  2  3  4]
  [ 5  6  7  8  9]]

 [[10 11 12 13 14]
  [15 16 17 18 19]]

 [[20 21 22 23 24]
  [25 26 27 28 29]]], shape=(3, 2, 5), dtype=int32)

rank_3_tensor[:, :, 4]
&gt;&gt;&gt;
&lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy=
array([[ 4,  9],
       [14, 19],
       [24, 29]])&gt;
</code></pre>
<p><img alt="image-20211023212032579" src="../tensorflow.assets/image-20211023212032579.png" /></p>
<h3 id="reshape">reshape</h3>
<p>flatten根据的是张量中元素在内存中的排列。TensorFlow uses C-style "row-major" memory ordering, where incrementing the rightmost index corresponds to a single step in memory.</p>
<p>在reshape时，要考虑到数据之间的关系，因为reshape只要求前后元素数目相同，不保证保留数据关系。</p>
<p><img alt="image-20211023212257813" src="../tensorflow.assets/image-20211023212257813.png" /></p>
<p><img alt="image-20211023212423647" src="../tensorflow.assets/image-20211023212423647.png" /></p>
<h3 id="broadcasting">broadcasting 特征</h3>
<p>这一点与numpy中的广播性质一致。</p>
<p>在一个operation当中，如果array与scalar进行运算，那么会发生什么呢？</p>
<p><img alt="A scalar is broadcast to match the shape of the 1-d array it is being multiplied to." src="../tensorflow.assets/broadcasting_1.svg" /></p>
<p>两个arrays进行operation，要求他们的维度是compatible的：</p>
<ul>
<li>维度相同</li>
<li>其中一个为1</li>
</ul>
<p>https://numpy.org/devdocs/user/basics.broadcasting.html</p>
<p>如：(5,4)可以与(1,)，(4,)进行operation，但不能与(3,)进行operation</p>
<p><img alt="image-20211023213825060" src="../tensorflow.assets/image-20211023213825060.png" /></p>
<p>从左侧开始配对，从最内的元素开始。</p>
<p><img alt="image-20211023214008872" src="../tensorflow.assets/image-20211023214008872.png" /></p>
<h3 id="_6">矩阵乘法</h3>
<p>实际上矩阵乘法就是符合广播特性的：</p>
<p>a = 1 * 3</p>
<p>b = 4 * 1</p>
<p>result = 4 * 3</p>
<h3 id="_7">小知识点：</h3>
<p>张量在图中流动，所以叫tensorflow</p>
<p><img alt="image-20211022221958949" src="../tensorflow.assets/image-20211022221958949.png" /></p>
<p>这个一维张量的流动过程</p>
<p>张量可以由numpy构建，以及一些特殊的方法</p>
<pre><code class="language-python">tf.zeros()
tf.ones()
tf.random.normal()
</code></pre>
<p>axis:</p>
<p><img alt="image-20211022230341291" src="../tensorflow.assets/image-20211022230341291.png" /></p>
<h2 id="_8">常用函数</h2>
<h3 id="tfvariable">tf.Variable</h3>
<p>用该函数标记后，该变量就是可训练的了。</p>
<pre><code class="language-python">w = tf.Variable(tf.random.normal([2,2],mean,stddev=1))
</code></pre>
<h3 id="_9">常用运算函数</h3>
<p>tf.add</p>
<p>tf.subtract</p>
<p>tf.multiply</p>
<p>等等</p>
<h3 id="_10">输入标签，标签</h3>
<p>tf.data.Dataset.from_tensor_slices</p>
<h3 id="tfgradienttape">tf.GradientTape</h3>
<p>计算梯度</p>
<h3 id="tfone_hot">tf.one_hot</h3>
<p>独热编码</p>
<h3 id="tfargmax">tf.argmax</h3>
<p>返回张量指定维度最大值的索引号。</p>
<h2 id="modelfit">model.fit()</h2>
<pre><code class="language-python">fit(x=None, 
    y=None, 
    batch_size=None, 
    epochs=1,               
    verbose='auto',
    callbacks=None,
    validation_split=0.0,
    validation_data=None, 
    shuffle=True,
    class_weight=None,
    sample_weight=None, 
    initial_epoch=0, 
    steps_per_epoch=None, 
    validation_steps=None, 
    validation_batch_size=None, 
    validation_freq=1,
    max_queue_size=10, 
    workers=1, 
    use_multiprocessing=False
)
</code></pre>
<p>输入数据x,y是最重要的两个参数，一般情况下直接将数据存到内存中，然后传到x就好。但是会出现这样的情况：</p>
<p><strong>数据太大，内存无法装下，这该怎么办呢？</strong></p>
<p><img alt="image-20211201212100958" src="../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20211201212100958.png" /></p>
<p>使用生成器generator或者tf.data.Dataset来进行输入。</p>
<h3 id="generatorx">用generator来做输入：x</h3>
<p>generator应该每次产生一个（x，y）。</p>
<p>并且fit()函数中y参数为None，batch_size也是None</p>
<p>也就是说生成器每次yield的x，y是一个batch。</p>
<p>案例：</p>
<p><img alt="img" src="../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/modb_20211123_6601cc88-4c4a-11ec-b512-fa163eb4f6be.png" /></p>
<h2 id="tfdata">tf.data</h2>
<p>tf.data.Dataset是核心</p>
<p>tf.data.Dataset是一个python迭代器</p>
<h3 id="_11">从内存中加载数据</h3>
<pre><code class="language-python">tf.data.Dataset.from_tensors()
&quot;&quot;&quot;Creates a `Dataset` with a single element, comprising the given tensors.
    `from_tensors` produces a dataset containing only a single element. To slice
    the input tensor into multiple elements, use `from_tensor_slices` instead.
    &quot;&quot;&quot;
tf.data.Dataset.from_tensor_slices()
&quot;&quot;&quot;Creates a `Dataset` whose elements are slices of the given tensors.
    The given tensors are sliced along their first dimension. This operation
    preserves the structure of the input tensors, removing the first dimension
    of each tensor and using it as the dataset dimension. All input tensors
    must have the same size in their first dimensions.
    &quot;&quot;&quot;
</code></pre>
<p>在第一个维度上进行切片，所以一般图片的第一维度是batch</p>
<h3 id="tfdatadatasetelement_spec">tf.data.Dataset.element_spec</h3>
<p>检查元素，要注意的是它检查的是单个元素：</p>
<pre><code class="language-python">dataset2 = tf.data.Dataset.from_tensor_slices(
   (tf.random.uniform([4]),
    tf.random.uniform([4, 100], maxval=100, dtype=tf.int32)))

dataset2.element_spec

&gt;&gt;&gt;
(TensorSpec(shape=(), dtype=tf.float32, name=None),
 TensorSpec(shape=(100,), dtype=tf.int32, name=None))
</code></pre>
<h3 id="datasetbatch">Dataset.batch</h3>
<p>Combines consecutive elements of this dataset into batches.</p>
<pre><code class="language-python">dataset = tf.data.Dataset.range(8)
dataset = dataset.batch(3)
list(dataset.as_numpy_iterator())
[[0,1,2],[3,4,5],[6,7]]
</code></pre>
<h2 id="tfnest">tf.nest</h2>
<p>nest模块是用于处理nested structure结构的，nested structure目前支持的结构是tuple，dict，namedtuple。</p>
<p>对于nested_structure可以使用is_nested()判断。</p>
<pre><code class="language-python">  tf.nest.is_nested(((7, 8), (5, 6)))
  &gt;&gt;&gt;True
  tf.nest.is_nested({&quot;a&quot;: 1, &quot;b&quot;: 2})
  &gt;&gt;&gt;True
  tf.nest.is_nested(set([1, 2]))
  &gt;&gt;&gt;False
  ones = tf.ones([2, 3])
  &gt;&gt;&gt;False
</code></pre>
<h3 id="functions">functions</h3>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/nest/assert_same_structure"><code>assert_same_structure(...)</code></a>: Asserts that two structures are nested in the same way.</p>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/nest/flatten"><code>flatten(...)</code></a>: Returns a flat list from a given nested structure.</p>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/nest/is_nested"><code>is_nested(...)</code></a>: Returns true if its input is a collections.abc.Sequence (except strings).</p>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/nest/map_structure"><code>map_structure(...)</code></a>: Applies <code>func</code> to each entry in <code>structure</code> and returns a new structure.</p>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/nest/pack_sequence_as"><code>pack_sequence_as(...)</code></a>: Returns a given flattened sequence packed into a given structure.</p>
<p><strong>常用函数是flatten()</strong></p>
<h2 id="tfnestflatten">tf.nest.flatten</h2>
<p>根据key来排序！</p>
<pre><code class="language-python">dict = { &quot;key3&quot;: &quot;value3&quot;, &quot;key1&quot;: &quot;value1&quot;, &quot;key2&quot;: &quot;value2&quot; }
tf.nest.flatten(dict)
&gt;&gt;&gt;  ['value1','value2','value3'] 
</code></pre>
<p>tuple根据内存顺序</p>
<h2 id="graph">graph</h2>
<p><img alt="image-20211022223035238" src="../tensorflow.assets/image-20211022223035238.png" /></p>
<h2 id="keras">keras</h2>
<p>八股文写代码方法：</p>
<pre><code class="language-python"># import 模块
# train test
# model
# model.compile
# model.fit
# model.summary
</code></pre>
<h3 id="sequential-model">sequential model</h3>
<pre><code class="language-python"># Define Sequential model with 3 layers
model = keras.Sequential(
    [
        layers.Dense(2, activation=&quot;relu&quot;, name=&quot;layer1&quot;),
        layers.Dense(3, activation=&quot;relu&quot;, name=&quot;layer2&quot;),
        layers.Dense(4, name=&quot;layer3&quot;),
    ]
)
# Call model on a test input
x = tf.ones((3, 3))
y = model(x)
</code></pre>
<p>sequential model 不适用于：</p>
<p>多输入与多输出</p>
<p>层之间的共享</p>
<p>等等。。。</p>
<h3 id="_12">网络结构</h3>
<p>拉直层</p>
<p>全连接层</p>
<p>卷积层</p>
<p>LSTM层</p>
<h3 id="modelcompile">model.compile</h3>
<pre><code class="language-python">model.compile(optimizer='sgd',loss,metrics=[&quot;accuracy&quot;])
</code></pre>
<p>指定网络的优化器与损失函数</p>
<h3 id="modelfit_1">model.fit</h3>
<pre><code class="language-python">model.fit(
    x,
    y,
    batch_size,
    validation_data,
    validation_split,
    validation_freq
)
</code></pre>
<h3 id="buildself-inputs_shape">build(self, inputs_shape)</h3>
<p>在继承tf.keras.layers.Layer时，可以重写call，build和init函数</p>
<p>build函数主要用于初始化参数，当call被第一次调用的时候，会先执行build()方法初始化变量，但后面再调用到call的时候，是不会再去执行build()方法初始化变量。一般用于需要知道输入tensor的形状，完成取决于输入tensor形状的初始化。</p>
<pre><code class="language-python">class Linear(keras.layers.Layer):
    def __init__(self, units=32):
        super(Linear, self).__init__()
        self.units = units

    def build(self, input_shape):
        self.w = self.add_weight(
            shape=(input_shape[-1], self.units),
            initializer=&quot;random_normal&quot;,
            trainable=True,
        )
        self.b = self.add_weight(
            shape=(self.units,), initializer=&quot;random_normal&quot;, trainable=True
        )

    def call(self, inputs):
        return tf.matmul(inputs, self.w) + self.b
</code></pre>
<h3 id="kerasbackendrnn">keras.backend.rnn</h3>
<p>在时间步维度上对张量进行迭代。</p>
<pre><code class="language-python">tf.keras.backend.rnn(
    step_function,
    inputs,
    initial_states,
    go_backwards=False,
    mask=None,
    constants=None,
    unroll=False,
    input_length=None,
    time_major=False,
    zero_output_for_mask=False,
)
</code></pre>
<p>需要定义step function：</p>
<pre><code class="language-python">step(input):
    &quot;&quot;&quot;do some thing&quot;&quot;&quot;
    return output
</code></pre>
<p>input张量需要去除time_steps维度，为[batch, features]</p>
<p>output张量同样没有时间维度，为[batch, output_dimensions]</p>
<h2 id="_13">模型的保存与加载</h2>
<p>一个模型包括多个部件：</p>
<ul>
<li>模型的结构，包括layers，how they are connected</li>
<li>权重，也就是模型的参数</li>
<li>优化器，在compile 模型的时候指定</li>
<li>损失函数与metric，也是在compile时候指定</li>
</ul>
<p>可以一次全部保存，也可以分别储存。</p>
<h3 id="_14">全部保存</h3>
<pre><code class="language-python"># 保存模型
model.save('path/to/location')
# 加载模型
model = keras.models.load_model('path/to/location')
</code></pre>
<p>model.save 保存所有，格式为<strong>SavedModel</strong>格式，也可以指定为<strong>H5</strong>格式</p>
<blockquote>
<p>SavedModel 其实是一个文件夹，保存了一些文件。详情可见</p>
<p>https://www.tensorflow.org/guide/saved_model#the_savedmodel_format_on_disk</p>
</blockquote>
<h3 id="_15">保存结构</h3>
<pre><code class="language-python">model.get_config()
model.from_config()

tf.keras.models.model_to_json()
tf.keras.models.model_from_json()
</code></pre>
<h3 id="_16">保存权重，参数</h3>
<pre><code class="language-python">model.save_weights()
model.load_weights()
</code></pre>
<p>可以存储的文件格式有：</p>
<ul>
<li>TensorFlow Checkpoint</li>
<li>HDF5</li>
<li>H5</li>
</ul>
<h1 id="pytorch">pytorch</h1>
<h2 id="_17">准备工作</h2>
<p>查看cudn的版本</p>
<pre><code class="language-python">nvcc --version
</code></pre>
<p>查看pytorch 版本</p>
<pre><code class="language-pytho">print(torch.__version__)
print(torch.cuda.is_available())
</code></pre>
<pre><code class="language-python">device = 'cuda' if torch.cuda.is_available() else 'cpu'
print('Using {} device'.format(device))
</code></pre>
<h2 id="torchutilsdatadataset">torch.utils.data.Dataset</h2>
<p>数据</p>
<ul>
<li>自带数据集</li>
<li>自定义数据集</li>
</ul>
<h3 id="_18">自定义数据集</h3>
<pre><code class="language-python">import os
import pandas as pd
from torchvision.io import read_image

class CustomImageDataset(Dataset):
    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):
        self.img_labels = pd.read_csv(annotations_file)
        self.img_dir = img_dir
        self.transform = transform
        self.target_transform = target_transform

    def __len__(self):
        return len(self.img_labels)

    def __getitem__(self, idx):
        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])
        image = read_image(img_path)
        label = self.img_labels.iloc[idx, 1]
        if self.transform:
            image = self.transform(image)
        if self.target_transform:
            label = self.target_transform(label)
        return image, label
</code></pre>
<p>Dataset是为file服务的。几个必须的函数：</p>
<ol>
<li>init</li>
<li>len</li>
<li>getitem</li>
</ol>
<p>用于之后传入dataloader</p>
<h2 id="torchutilsdatadataloader">torch.utils.data.DataLoader</h2>
<p>这是一个可迭代类，为了就是将dataset打包成一个个batch，用于训练</p>
<p>所以说 dataloader是为训练服务的。</p>
<h2 id="nnmodule">nn.module</h2>
<pre><code class="language-python">class NeuralNetwork(nn.Module):
    def __init__(self):
        super(NeuralNetwork, self).__init__()
        self.flatten = nn.Flatten()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(28*28, 512),
            nn.ReLU(),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Linear(512, 10),
        )

    def forward(self, x):
        x = self.flatten(x)
        logits = self.linear_relu_stack(x)
        return logits

model = NeuralNetwork().to(device)
</code></pre>
<p>在init中定义组件，在forward函数定义网络的前向传播</p>
<h2 id="nnmodulelayer">nn.module中的layer</h2>
<pre><code class="language-python">nn.Flatten
nn.Linear
nn.ReLU
nn.Sequential   layer的容器
nn.Softmax
</code></pre>
<h2 id="_19">常用函数</h2>
<pre><code class="language-python">for name, param in model.named_parameters():
    print(f&quot;Layer: {name} | Size: {param.size()} | Values : {param[:2]} \n&quot;)
</code></pre>
<h2 id="_20">训练网络</h2>
<ol>
<li>定义损失函数</li>
<li>定义优化器</li>
<li>定义一个epoch的行为</li>
<li>开始训练多个epoch</li>
</ol>
<pre><code class="language-python"># 1
loss_fn = nn.CrossEntropyLoss()
# 2 
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
# 3 
def train_loop(dataloader, model, loss_fn, optimizer):
    size = len(dataloader.dataset)
    for batch, (X, y) in enumerate(dataloader):
        # Compute prediction and loss
        pred = model(X)
        loss = loss_fn(pred, y)

        # Backpropagation
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if batch % 100 == 0:
            loss, current = loss.item(), batch * len(X)
            print(f&quot;loss: {loss:&gt;7f}  [{current:&gt;5d}/{size:&gt;5d}]&quot;)


def test_loop(dataloader, model, loss_fn):
    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    test_loss, correct = 0, 0

    with torch.no_grad():
        for X, y in dataloader:
            pred = model(X)
            test_loss += loss_fn(pred, y).item()
            correct += (pred.argmax(1) == y).type(torch.float).sum().item()

    test_loss /= num_batches
    correct /= size
    print(f&quot;Test Error: \n Accuracy: {(100*correct):&gt;0.1f}%, Avg loss: {test_loss:&gt;8f} \n&quot;)

loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)

epochs = 10
for t in range(epochs):
    print(f&quot;Epoch {t+1}\n-------------------------------&quot;)
    train_loop(train_dataloader, model, loss_fn, optimizer)
    test_loop(test_dataloader, model, loss_fn)
print(&quot;Done!&quot;)    
</code></pre>
<p>Call <code>optimizer.zero_grad()</code> to reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration.</p>
<h2 id="_21">反向传播</h2>
<p>输入x，输出y，参数为<span class="arithmatex">\(\theta\)</span></p>
<p>定义损失函数 <span class="arithmatex">\(J(\theta)\)</span></p>
<p>我们需要计算的是 $\nabla _\theta J(\theta) $</p>
<h3 id="_22">计算图：</h3>
<p>一个DAG，一个节点可以是变量，也可以是一个operation。</p>
<h3 id="_23">微积分中的链式法则</h3>
<p>对于这样一个函数：
$$
y = g(x)\
z = f(g(x))
$$
z对x求导，写为：
$$
dz/dx = dz/dy*dy/dx
$$
这只是标量的写法，如果扩展为向量：
$$
\nabla_x z = (\frac{\delta y}{\delta x})^T\nabla_y z
$$
其中z是一个标量 <span class="arithmatex">\(x \in R^m, y \in R^n\)</span>, <span class="arithmatex">\(\frac{\delta y}{\delta x}\)</span> 是 n<span class="arithmatex">\(\times\)</span>m的 Jacobian矩阵。</p>
<p>此时如果我们将 z看作：损失函数输出的标量。
$$
z = f(y)
$$
y看作神经网络的输出量，g()是一个神经网络：
$$
y = g(x)
$$
在训练过程中，输入实际上是网络参数<span class="arithmatex">\(x\)</span>, 通过不断改变<span class="arithmatex">\(x\)</span> 使得损失函数最小。</p>
<h3 id="_24">第一个版本的反向传播</h3>
<p>定义这样一个forward过程：</p>
<p><span class="arithmatex">\(u^{(1)},...,u^{(n_i)}\)</span> 为输入节点（可以理解为参数w）</p>
<p><span class="arithmatex">\(u^{(n)}\)</span>为输出，是损失函数计算出来的标量值。</p>
<p><img alt="image-20211026122804916" src="../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20211026122804916.png" /></p>
<p>目标，求
$$
\frac{\partial u^{(n)}}{\partial u^{(j)}},j\in{1,2,3...i}\
=\sum_{i: j \in P a\left(u^{(i)}\right)} \frac{\partial u^{(n)}}{\partial u^{(i)}} \frac{\partial u^{(i)}}{\partial u^{(j)}}
$$
执行反向传播所需要的计算量与G中边的数量成比例。反向传播算法的设计就是为了减少公共子表达式的数量，而不考虑储存的开销。</p>
<p><img alt="image-20211026212439335" src="../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20211026212439335.png" /></p>
<p>算法的简化版本，只适用于前面提到的前向传播过程。大致观察一下，可以看到新增了一个grad_table，用于存储已经计算过了的梯度，这就是一种牺牲空间换取时间的算法。</p>
<h3 id="_25">用例子来说明反向传播算法</h3>
<p><img src="%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20211026212848355.png" alt="image-20211026212848355" style="zoom:50%;" /></p>
<p>对于这样一个图结构，记：<span class="arithmatex">\(j &lt; i , u_j是u_i的父节点\)</span></p>
<ol>
<li><span class="arithmatex">\(\frac{\partial u^{6}}{\partial u^{6}}=1\)</span> 直接记下，写入grad_table</li>
<li>进入迭代，j = 5</li>
<li>（若无grad_table）计算<span class="arithmatex">\(\frac{\partial u^{6}}{\partial u^{5}}=\frac{\partial u^{6}}{\partial u^{5}}\)</span> </li>
<li>有grad_table，<span class="arithmatex">\(grad\_table[u^6]=1\)</span>，所以<span class="arithmatex">\(grad\_table[u^5]=1\times\frac{\partial u^{6}}{\partial u^{5}}\)</span></li>
<li>进入迭代，j=4</li>
<li>（若无grad_table）计算<span class="arithmatex">\(\frac{\partial u^{6}}{\partial u^{4}}=\frac{\partial u^{6}}{\partial u^{4}}\)</span></li>
<li>有grad_table，<span class="arithmatex">\(grad\_table[u^6]=1\)</span>，所以<span class="arithmatex">\(grad\_table[u^4]=1\times\frac{\partial u^{6}}{\partial u^{4}}\)</span></li>
<li>进入迭代，j=3</li>
<li>（若无grad_table）计算<span class="arithmatex">\(\frac{\partial u^{6}}{\partial u^{3}}=\frac{\partial u^{6}}{\partial u^{5}}\frac{\partial u^{5}}{\partial u^{3}}\)</span></li>
<li>有grad_table，<span class="arithmatex">\(grad\_table[u^5]=1\times\frac{\partial u^{6}}{\partial u^{5}}\)</span>，所以<span class="arithmatex">\(grad\_table[u^3]=grad\_table[u^5]\times\frac{\partial u^{5}}{\partial u^{3}}\)</span></li>
<li>进入迭代，j=2</li>
<li>（若无grad_table）计算<span class="arithmatex">\(\frac{\partial u^{6}}{\partial u^{2}}=\frac{\partial u^{6}}{\partial u^{4}}\frac{\partial u^{4}}{\partial u^{2}}+\frac{\partial u^{6}}{\partial u^{5}}\frac{\partial u^{5}}{\partial u^{2}}\)</span></li>
<li>有grad_table，所以<span class="arithmatex">\(grad\_table[u^2]=grad\_table[u^5]\times\frac{\partial u^{5}}{\partial u^{2}}+grad\_table[u^4]\times\frac{\partial u^{4}}{\partial u^{2}}\)</span></li>
<li>以此类推</li>
</ol>
<p>所以综上所述，节约了多次内积的计算时间,但是这只是一种简单且直观的情况。</p>
<h3 id="_26">真正地考虑一个神经网络模型：</h3>
<p>它的forward过程如下描述：</p>
<p><img alt="image-20211026214705162" src="../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20211026214705162.png" /></p>
<p>反向传播过程：</p>
<p><img alt="image-20211026214837003" src="../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20211026214837003.png" /></p>
<h3 id="reference">reference</h3>
<p>https://github.com/exacity/deeplearningbook-chinese</p>
<h1 id="rnn">RNN循环神经网络</h1>
<p>本质上，任何包含循环的函数都可以被认为是一个循环神经网络。
$$
\boldsymbol{h}^{(t)}=f\left(\boldsymbol{h}^{(t-1)}, \boldsymbol{x}^{(t)} ; \boldsymbol{\theta}\right)
$$
网络通常使用<span class="arithmatex">\(\boldsymbol{h}^{(t)}\)</span> 作为过去序列与任务相关方面的有损摘要，一般而言一定是有损的。<span class="arithmatex">\(\boldsymbol{h}^{(t)}\)</span> 映射任意长度序列<span class="arithmatex">\(\left(\boldsymbol{x}^{(t)}, \boldsymbol{x}^{(t-1)}, \boldsymbol{x}^{(t-2)}, \ldots, \boldsymbol{x}^{(2)}, \boldsymbol{x}^{(1)}\right)\)</span>到一定长度<span class="arithmatex">\(\boldsymbol{h}^{(t)}\)</span> </p>
<p><img alt="image-20211023151547593" src="../tensorflow.assets/image-20211023151547593.png" /></p>
<p>计算公式：
$$
h_t = tanh(w_xx_t+w_hh_{t-1}+b)  \
y_t = w_yh_t
$$
其中整个模型共享参数：<span class="arithmatex">\(w_h,w_x,w_y\)</span></p>
<h2 id="rnn_1">经典的RNN结构</h2>
<p>n-&gt;n
n-&gt;1
n-&gt;m</p>
<p><img alt="image-20211023200954584" src="../tensorflow.assets/image-20211023200954584.png" /></p>
<h2 id="_27">优缺点</h2>
<p>优点：</p>
<p>• Possibility of processing input of any length
• Model size not increasing with size of input
• Computation takes into account historical information
• Weights are shared across time</p>
<h2 id="_28">缺点：</h2>
<p>• Computation being slow
• Difficulty of accessing information from a long time ago
• Cannot consider any future input for the current state</p>
<p>损失函数
$$
\mathcal{L}(\hat{y}, y)=\sum_{t=1}^{T_{y}} \mathcal{L}\left(\widehat{y}^{<t>}, y^{<t>}\right)
$$</p>
<p>如果我们需要的模型是n-&gt;1：</p>
<p><img alt="image-20211023201258955" src="../tensorflow.assets/image-20211023201258955.png" /></p>
<p>那么训练所给的数据也应该如：
$$
[X^n,y^n]\
X^n=x_1,x_2,...,x_t\
y^n=y_t
$$</p>
<h2 id="_29">损失函数：</h2>
<p>根据时间步的连乘。
$$
\frac{\partial \mathcal{L}^{(T)}}{\partial W}=\left.\sum_{t=1}^{T} \frac{\partial \mathcal{L}^{(T)}}{\partial W}\right|_{(t)}
$$</p>
<p>常见问题: vanishing/ exploding gradient</p>
<p>The reason why they happen is that it is difficult to capture long term dependencies because of multiplicative gradient that can be exponentially decreasing/increasing with respect to the number of layers. 时间步一长，连乘就要出问题。</p>
<p>对梯度的最大值进行限制是一个放置exploding的方法。</p>
<h2 id="reference_1">reference</h2>
<p>https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks#overview</p>
<h2 id="sequence-to-sequence">sequence to sequence 结构</h2>
<p>一般RNN结构对于输入与输出的个数都做了限定，但实际上很多任务的序列长度是不固定的。</p>
<p><img alt="img" src="../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/20030902-f3b85bfbaae7d8b1.png" /></p>
<p><img alt="img" src="../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/20030902-479f2b802293c8c5.png" /></p>
<p><img alt="img" src="../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/20030902-98c17bbce81a14ba.png" /></p>
<h3 id="_30">分析</h3>
<p>可以观察到，其实前半部分是一个encoder模型，也可以理解为一个N-&gt;1的RNN模型。这个向量一般称作<strong>上下文向量c</strong></p>
<p>当然c可以由<span class="arithmatex">\(h_n\)</span>决定，也可以由<span class="arithmatex">\(h_1,...h_n\)</span>决定。
$$
\begin{gathered}
c=h_{N} \
c=q\left(h_{N}\right) \
c=q\left(h_{1}, h_{2}, \ldots, h_{N}\right)
\end{gathered}
$$
然后使用decoder对c进行解码：</p>
<p><img alt="img" src="../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/20030902-6f461e520d31cb27.png" /></p>
<p><img alt="img" src="../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/20030902-177f158afe2216bf.png" /></p>
<p><img alt="img" src="../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/20030902-840078e115f43250.png" /></p>
<h3 id="teacher-forcing">teacher forcing</h3>
<p>TODO</p>
<h3 id="attention">attention</h3>
<p>以上模型的缺陷：</p>
<ul>
<li>c如果定长，很难完全涵盖输入</li>
<li>RNN 存在长序列梯度消失的问题，只使用最后一个神经元得到的向量 <strong>c</strong> 效果不理想。</li>
<li>与人类的注意力方式不同，即人类在阅读文章的时候，会把注意力放在当前的句子上。</li>
</ul>
<p>使用attention机制，使得上下文向量c不再固定，而是根据当前信息计算c</p>
<p><img alt="img" src="../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/20030902-3caa6122e9b613c5.png" /></p>
<p><img alt="image-20211025172732026" src="../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20211025172732026.png" /></p>
<p>上下文向量---&gt;受到拼接</p>
<h4 id="keraslayerssimplernn">keras.layers.SimpleRNN</h4>
<p>对于一个循环神经网络，它的输入张量一般为：</p>
<pre><code>(samples，timesteps,features)
</code></pre>
<p>例如手上的数据为：一小时一个时间戳，一个样本统计了100h，涉及到12个变量，统计了80次，那么数据可以写为：</p>
<p>(80,100,12)</p>
<p>为了方便之后参数计算的讨论，我们考虑的情况为</p>
<p>x.shape = (10,5,2)</p>
<p>y.shape = (10,2) </p>
<p>来做一个二分类问题。</p>
<h3 id="simplernn">SimpleRNN 的输入输出</h3>
<ul>
<li><strong>它的输入被指定为3维(samples，timesteps, features)</strong></li>
</ul>
<pre><code class="language-python">x = tf.random.normal(shape=(3,10,5,4))
rnn = layers.SimpleRNN(units=1)
y = rnn(x)
print(y.shape)
&gt;&gt;&gt;
ValueError: Input 0 of layer simple_rnn is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (3, 10, 5, 4)
</code></pre>
<ul>
<li>输出维度由唯一的参数units决定</li>
</ul>
<p>units: Positive integer, dimensionality of the output space。很显然，对于一个batch，他的输出为(batch_size,units)</p>
<h3 id="units">units到底是什么呢</h3>
<div class="arithmatex">\[
h_t = tanh(w_xx_t+w_hh_{t-1}+b)
\]</div>
<p>参数共享，所以看到的展开式，都是共享这一套参数。units，本质上是<span class="arithmatex">\(h_t\)</span>的维度。</p>
<p><span class="arithmatex">\((5,2)\)</span>的x进来，实际上就是一个两维的向量进来，输出的<span class="arithmatex">\(h_t\)</span>也是一个两位向量，所以这样计算：</p>
<p><img alt="image-20211027211055617" src="../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20211027211055617.png" /></p>
<p>一共是10个参数。</p>
<p><img alt="image-20211023151547593" src="../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20211023151547593.png" /></p>
<p>注意，图中的<span class="arithmatex">\(w_y\)</span>并不是RNN结构中参数，因为SimpleRNN中就不包括y，他直接将h向量输出，如果需要图中的y，需要自行添加dense层。</p>
<p>公式为：</p>
<p><strong>recurrent_weights</strong>+ <strong>input_weights</strong> + <strong>biases</strong></p>
<p><em>num_units* num_units + num_features* num_units + biases</em></p>
<p>这张图片也方便理解:</p>
<p><img alt="enter image description here" src="../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/71An9.png" /></p>
<blockquote>
<p>https://stackoverflow.com/questions/50134334/number-of-parameters-for-keras-simplernn</p>
</blockquote>
<h2 id="lstm">LSTM</h2>
<p>LSTM的全称是Long Short Term Memory，顾名思义，它具有记忆长短期信息的能力的神经网络。LSTM首先在1997年由Hochreiter &amp; Schmidhuber 提出，由于深度学习在2012年的兴起，LSTM又经过了若干代大牛(Felix Gers, Fred Cummins, Santiago Fernandez, Justin Bayer, Daan Wierstra, Julian Togelius, Faustino Gomez, Matteo Gagliolo, and Alex Gloves)的发展，由此便形成了比较系统且完整的LSTM框架，并且在很多领域得到了广泛的应用。本文着重介绍深度学习时代的LSTM。</p>
<p>LSTM提出的动机是为了解决上面我们提到的长期依赖问题。传统的RNN节点输出仅由权值，偏置以及激活函数决定（图3）。RNN是一个链式结构，每个时间片使用的是相同的参数。</p>
<blockquote>
<p>https://zhuanlan.zhihu.com/p/42717426</p>
</blockquote>
<p><img alt="img" src="../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/v2-e1cb116af01ef77826cd55bc1f8e5dd9_720w.jpg" /></p>
<p><img alt="img" src="../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/v2-d7fc6f5ee5dd07d2662bceca25488fe5_720w.jpg" /></p>
<p>LSTM使用门的机制，控制特征的流通和损失。LSTM可以做到t9的时刻考虑t2的特征，</p>
<h3 id="cell-state">cell state</h3>
<p><img alt="img" src="../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/v2-3d8d2ff15f8e15e231b9c1d7338f1be7_720w.jpg" />
$$
C_{t}=f_{t} \times C_{t-1}+i_{t} \times \tilde{C}_{t}
$$
<img alt="img" src="../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/v2-89ddea95073d6cb76623af1e33fbd3c3_720w.jpg" /></p>
<p><span class="arithmatex">\(f_{t}\)</span>是一个遗忘门，0~1之间</p>
<p><img alt="img" src="../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/v2-950218fd228d5d0fd36c798eb1485e3a_720w.jpg" /></p>
<p><span class="arithmatex">\(i_{t}\)</span>是输入门，其功能非常相似用于保留与以往，<span class="arithmatex">\(h_t,x_t\)</span>的信息</p>
<p><span class="arithmatex">\(\tilde{C}_{t}\)</span>表示单元状态更新值</p>
<p><img alt="img" src="../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/v2-9f2646898e9d3f82fd022735b8ec6f80_720w.jpg" /></p>
<p><span class="arithmatex">\(C_t\)</span>为cell状态</p>
<p><img alt="img" src="../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/v2-3edbdda4409cb1774c03bb459fa4a6e5_720w.jpg" /></p>
<p>由根据<span class="arithmatex">\(o_t\)</span>输出门，更新的 <span class="arithmatex">\(h_t\)</span> 值。</p>
<p>综上所述，LSTM一个cell里面输出了两个变量，分别为<span class="arithmatex">\(C_t\)</span>，<span class="arithmatex">\(h_t\)</span>。</p>
<h3 id="keraslayerslstm">keras.layers.LSTM</h3>
<pre><code class="language-python">&gt;&gt;&gt; inputs = tf.random.normal([32, 10, 8])
&gt;&gt;&gt; lstm = tf.keras.layers.LSTM(4)
&gt;&gt;&gt; output = lstm(inputs)
&gt;&gt;&gt; print(output.shape)
  (32, 4)
&gt;&gt;&gt; lstm = tf.keras.layers.LSTM(4, return_sequences=True, return_state=True)
&gt;&gt;&gt; whole_seq_output, final_memory_state, final_carry_state = lstm(inputs)
&gt;&gt;&gt; print(whole_seq_output.shape)
  (32, 10, 4)
&gt;&gt;&gt; print(final_memory_state.shape)
  (32, 4)
&gt;&gt;&gt; print(final_carry_state.shape)
  (32, 4)
</code></pre>
<p>units: <span class="arithmatex">\(h_t\)</span> 的维度</p>
<pre><code class="language-python">whole_seq_output, final_memory_state, final_carry_state = lstm(inputs)
</code></pre>
<p>注意输出的顺序</p>
<p>sequence_output 是指 <span class="arithmatex">\(h_t\)</span></p>
<p>whole_seq_output 是指 <span class="arithmatex">\(h_1,h_2,...h_t\)</span></p>
<h3 id="tensorflow2-lstm">tensorflow2 的 LSTM 源码阅读</h3>
<p>LSTM是一个layer，它继承了RNN layer</p>
<p>使用的是LSTMCell</p>
<p>LSTM是一个循环操作（输入三维度张量），LSTMCell就是一次循环的操作（输入二维张量，去除了时间戳）。</p>
<p><img alt="image-20211110100618256" src="../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20211110100618256.png" /></p>
<p><img alt="image-20211110100647980" src="../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20211110100647980.png" /></p>
<h4 id="lstmcell">LSTMCell 类</h4>
<p><img alt="image-20211110100743904" src="../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20211110100743904.png" /></p>
<pre><code class="language-python">class LSTMCell(DropoutRNNCellMixin, Layer):
  &quot;&quot;&quot;Cell class for the LSTM layer.

  Args:
    units: Positive integer, dimensionality of the output space.
    activation: Activation function to use.
      Default: hyperbolic tangent (`tanh`).
      If you pass `None`, no activation is applied
      (ie. &quot;linear&quot; activation: `a(x) = x`).
    recurrent_activation: Activation function to use
      for the recurrent step.
      Default: hard sigmoid (`hard_sigmoid`).
      If you pass `None`, no activation is applied
      (ie. &quot;linear&quot; activation: `a(x) = x`).
    use_bias: Boolean, whether the layer uses a bias vector.
    kernel_initializer: Initializer for the `kernel` weights matrix,
      used for the linear transformation of the inputs.
    recurrent_initializer: Initializer for the `recurrent_kernel`
      weights matrix,
      used for the linear transformation of the recurrent state.
    bias_initializer: Initializer for the bias vector.
    unit_forget_bias: Boolean.
      If True, add 1 to the bias of the forget gate at initialization.
      Setting it to true will also force `bias_initializer=&quot;zeros&quot;`.
      This is recommended in [Jozefowicz et al., 2015](
        http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)
    kernel_regularizer: Regularizer function applied to
      the `kernel` weights matrix.
    recurrent_regularizer: Regularizer function applied to
      the `recurrent_kernel` weights matrix.
    bias_regularizer: Regularizer function applied to the bias vector.
    kernel_constraint: Constraint function applied to
      the `kernel` weights matrix.
    recurrent_constraint: Constraint function applied to
      the `recurrent_kernel` weights matrix.
    bias_constraint: Constraint function applied to the bias vector.
    dropout: Float between 0 and 1.
      Fraction of the units to drop for
      the linear transformation of the inputs.
    recurrent_dropout: Float between 0 and 1.
      Fraction of the units to drop for
      the linear transformation of the recurrent state.

  Call arguments:
    inputs: A 2D tensor.
    states: List of state tensors corresponding to the previous timestep.
    training: Python boolean indicating whether the layer should behave in
      training mode or in inference mode. Only relevant when `dropout` or
      `recurrent_dropout` is used.
  &quot;&quot;&quot;
</code></pre>
<p>一个Cell只需要输入二维张量[batch,features]</p>
<p>call 函数返回值为  h, [h,c]</p>
<pre><code class="language-python">def call(self, inputs, states, training=None):
    h_tm1 = states[0]  # previous memory state
      c_tm1 = states[1]  # previous carry state
    &quot;&quot;&quot;
    省略
    &quot;&quot;&quot;
      return h, [h, c]
</code></pre>
<p><img src="%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20211110101849995.png" alt="image-20211110101849995"  /></p>
<p>正如这一个Cell所示，进去一个<span class="arithmatex">\(x_t\)</span> 出来 <span class="arithmatex">\(h_t,c_t\)</span></p>
<h4 id="lstm_1">LSTM 类</h4>
<pre><code class="language-python">class LSTM(RNN):
    def __init__(self,units,**kwargs):
        &quot;&quot;&quot;
        省略
        &quot;&quot;&quot;
        cell = LSTMCell(units,**cell_kwargs)
        super(LSTM, self).__init__(cell,**kwargs)

    def call(self, inputs):
        return super(LSTM, self).call(inputs)
</code></pre>
<p>可以看到，本质上是使用了RNN的call函数，但是把Cell进行了更改。</p>
<p>前面已经提到了Cell是一个：</p>
<p>[batch, features] -----LSTMCell-----&gt; h, [h, c]</p>
<p>接下来进入RNN的call函数中：</p>
<pre><code class="language-python">    def call(self,inputs,):
    # The input should be dense, padded with zeros. If a ragged input is fed
    # into the layer, it is padded and the row lengths are used for masking.
    &quot;&quot;&quot;
    进行一系列检查
    &quot;&quot;&quot;
        inputs,initial_state,constants=
        self._process_inputs(inputs,initial_state,constants)
        # 定义step函数，执行Cell的call function
        def step(inputs, states):
            states = states[0] 
            if len(states) == 1 and is_tf_rnn_cell else states
            output, new_states = cell_call_fn(inputs, states, **kwargs)
            if not tf.nest.is_nested(new_states):
                  new_states = [new_states]
            return output, new_states
        # 将step函数放到K.rnn中执行
        last_output, outputs, states = backend.rnn(
        step,
        inputs,
        initial_state)
</code></pre>
<h4 id="transformer">transformer中的位置编码</h4>
<p>一种好的位置编码方案需要满足以下几条要求：  </p>
<p>它能为每个时间步输出一个独一无二的编码； </p>
<p>不同长度的句子之间，任何两个时间步之间的距离应该保持一致； </p>
<p>模型应该能毫不费力地泛化到更长的句子。</p>
<p>它的值应该是有界的； </p>
<p>它必须是确定性的。</p>

              
            </article>
          </div>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
        
        <a href="../%E3%80%8A%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%8E%9F%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/" class="md-footer__link md-footer__link--prev" aria-label="Previous: 《深入浅出强化学习：原理入门》" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              《深入浅出强化学习：原理入门》
            </div>
          </div>
        </a>
      
      
        
        <a href="../%E3%80%8A%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" class="md-footer__link md-footer__link--next" aria-label="Next: 《算法导论》阅读笔记" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              《算法导论》阅读笔记
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../..", "features": [], "search": "../../assets/javascripts/workers/search.5e67fbfe.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.c44cc438.min.js"></script>
      
        <script src="../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>